# ğŸ“– å®Œæ•´ä½¿ç”¨ç¤ºä¾‹

## 1. è®­ç»ƒæ¨¡å‹ç¤ºä¾‹

### åŸºç¡€è®­ç»ƒ
```bash
# 1. å‡†å¤‡æ•°æ®é›†
python dataset/prepare_data.py \
  --input-dir ./raw_data \
  --output-dir ./data/processed \
  --format coco

# 2. å¼€å§‹è®­ç»ƒ
python train.py \
  --config configs/train_config.yaml
```

### é«˜çº§è®­ç»ƒé…ç½®
```python
# custom_train.py
import torch
from pathlib import Path
import yaml

from dataset.data_loader import create_dataloaders, DatasetConfig
from models.convnext_upernet import ConvNeXtUPerNet
from training.losses import CombinedLoss
from training.trainer import Trainer

# 1. åŠ è½½é…ç½®
with open('configs/train_config.yaml') as f:
    config = yaml.safe_load(f)

# 2. åˆ›å»ºæ•°æ®åŠ è½½å™¨
dataset_config = DatasetConfig(**config['dataset'])
train_loader, val_loader = create_dataloaders(
    config=dataset_config,
    batch_size=16,
    num_workers=4
)

# 3. åˆ›å»ºæ¨¡å‹
model = ConvNeXtUPerNet(
    encoder_name='convnext_small',  # ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹
    pretrained=True,
    num_classes=1,
    decoder_channels=512,  # å¢åŠ é€šé“æ•°
    deep_supervision=True,
    edge_branch=True
)

# 4. åˆ›å»ºæŸå¤±å’Œä¼˜åŒ–å™¨
loss_fn = CombinedLoss(
    dice_weight=0.4,
    focal_weight=0.3,
    bce_weight=0.2,
    boundary_weight=0.1,
    use_tversky=True,  # ä½¿ç”¨Tverskyå¤„ç†æ¼æ£€
    tversky_beta=0.7
)

optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=1e-3,
    weight_decay=1e-4
)

scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
    optimizer,
    T_max=200,
    eta_min=1e-6
)

# 5. åˆ›å»ºè®­ç»ƒå™¨
trainer = Trainer(
    model=model,
    optimizer=optimizer,
    loss_fn=loss_fn,
    scheduler=scheduler,
    device='cuda',
    output_dir='./outputs',
    use_amp=True,
    use_ema=True,
    use_swa=True,
    swa_start_epoch=180
)

# 6. è®­ç»ƒ
trainer.train(
    train_loader=train_loader,
    val_loader=val_loader,
    num_epochs=200,
    early_stopping_patience=20
)
```

## 2. æ¨ç†ç¤ºä¾‹

### å•å¼ å›¾åƒæ¨ç†
```python
import torch
import cv2
from models.convnext_upernet import ConvNeXtUPerNet

# 1. åŠ è½½æ¨¡å‹
model = ConvNeXtUPerNet(
    encoder_name='convnext_tiny',
    pretrained=False
)
checkpoint = torch.load('outputs/best.pth')
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()
model.cuda()

# 2. åŠ è½½å›¾åƒ
image = cv2.imread('test_image.jpg')
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# 3. é¢„å¤„ç†
import albumentations as A
from albumentations.pytorch import ToTensorV2

transform = A.Compose([
    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ToTensorV2()
])

transformed = transform(image=image)
image_tensor = transformed['image'].unsqueeze(0).cuda()

# 4. æ¨ç†
with torch.no_grad():
    outputs = model(image_tensor)
    pred = torch.sigmoid(outputs['out'])
    pred_mask = (pred > 0.5).float()

# 5. åå¤„ç†
pred_mask = pred_mask[0, 0].cpu().numpy() * 255
pred_mask = pred_mask.astype('uint8')

# 6. ä¿å­˜ç»“æœ
cv2.imwrite('result.png', pred_mask)
```

### é«˜åˆ†è¾¨ç‡å›¾åƒæ¨ç†ï¼ˆæ»‘çª—ï¼‰
```python
import torch
from models.convnext_upernet import ConvNeXtUPerNet
from inference.sliding_window import SlidingWindowInference
import cv2
import numpy as np

# 1. åŠ è½½æ¨¡å‹
model = ConvNeXtUPerNet(encoder_name='convnext_tiny', pretrained=False)
checkpoint = torch.load('outputs/best.pth')
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()
model.cuda()

# 2. åˆ›å»ºæ»‘çª—æ¨ç†å™¨
sliding_window = SlidingWindowInference(
    window_size=(1024, 1024),
    overlap=0.25,
    batch_size=4,
    blend_mode='gaussian'
)

# 3. åŠ è½½é«˜åˆ†è¾¨ç‡å›¾åƒ
image = cv2.imread('high_res_image.jpg')  # ä¾‹å¦‚ 4096x4096
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# 4. é¢„å¤„ç†
import albumentations as A
from albumentations.pytorch import ToTensorV2

transform = A.Compose([
    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ToTensorV2()
])

image_tensor = transform(image=image)['image']

# 5. æ»‘çª—æ¨ç†
pred = sliding_window(model, image_tensor, device='cuda')

# 6. åå¤„ç†
pred_mask = (pred[0] > 0.5).float().cpu().numpy() * 255
pred_mask = pred_mask.astype('uint8')

# 7. ä¿å­˜
cv2.imwrite('high_res_result.png', pred_mask)
```

### TTAæ¨ç†ï¼ˆæœ€é«˜ç²¾åº¦ï¼‰
```python
from inference.sliding_window import TTAInference

# 1. åˆ›å»ºTTAæ¨ç†å™¨
tta = TTAInference(
    scales=[0.75, 1.0, 1.25],  # å¤šå°ºåº¦
    flip_h=True,                # æ°´å¹³ç¿»è½¬
    flip_v=True,                # å‚ç›´ç¿»è½¬
    use_sliding_window=True,    # ç»“åˆæ»‘çª—
    window_config={
        'window_size': (1024, 1024),
        'overlap': 0.25,
        'batch_size': 4
    }
)

# 2. TTAæ¨ç†ï¼ˆ6æ¬¡å¢å¼ºå¹³å‡ï¼‰
pred = tta(model, image_tensor, device='cuda')

# 3. åå¤„ç†
pred_mask = (pred[0] > 0.5).float().cpu().numpy() * 255
pred_mask = pred_mask.astype('uint8')
```

## 3. æ‰¹é‡æ¨ç†ç¤ºä¾‹

```python
import torch
from torch.utils.data import DataLoader
from tqdm import tqdm
import cv2
from pathlib import Path

# 1. å‡†å¤‡æµ‹è¯•é›†
from dataset.data_loader import CrackDataset, DatasetConfig, get_validation_augmentation

config = DatasetConfig(
    root='./data/processed',
    split='test',
    crop_size=(512, 512)
)

test_dataset = CrackDataset(
    config,
    transform=get_validation_augmentation(config)
)

test_loader = DataLoader(
    test_dataset,
    batch_size=8,
    shuffle=False,
    num_workers=4
)

# 2. åŠ è½½æ¨¡å‹
model = ConvNeXtUPerNet(encoder_name='convnext_tiny', pretrained=False)
checkpoint = torch.load('outputs/best.pth')
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()
model.cuda()

# 3. æ‰¹é‡æ¨ç†
output_dir = Path('results')
output_dir.mkdir(exist_ok=True)

all_ious = []

with torch.no_grad():
    for batch in tqdm(test_loader):
        images = batch['image'].cuda()
        masks = batch['mask'].cuda()
        ids = batch['id']
        
        # æ¨ç†
        outputs = model(images)
        preds = torch.sigmoid(outputs['out'])
        pred_masks = (preds > 0.5).float()
        
        # è®¡ç®—IoU
        intersection = (pred_masks * masks).sum(dim=[1, 2, 3])
        union = (pred_masks + masks).clamp(0, 1).sum(dim=[1, 2, 3])
        iou = (intersection / (union + 1e-8)).cpu().numpy()
        all_ious.extend(iou.tolist())
        
        # ä¿å­˜ç»“æœ
        for i, sample_id in enumerate(ids):
            pred_mask = pred_masks[i, 0].cpu().numpy() * 255
            pred_mask = pred_mask.astype('uint8')
            
            cv2.imwrite(str(output_dir / f'{sample_id}_pred.png'), pred_mask)

# 4. ç»Ÿè®¡
print(f"Mean IoU: {np.mean(all_ious):.4f}")
print(f"Std IoU: {np.std(all_ious):.4f}")
```

## 4. æ¨¡å‹å¯¼å‡ºç¤ºä¾‹

### å¯¼å‡ºONNX
```bash
# å®Œæ•´æµç¨‹ï¼šéªŒè¯ + ä¼˜åŒ– + é‡åŒ– + åŸºå‡†æµ‹è¯•
python export_onnx.py \
  --checkpoint outputs/best.pth \
  --output model.onnx \
  --input-shape 1 3 512 512 \
  --verify \
  --optimize \
  --quantize \
  --benchmark
```

### ONNXæ¨ç†
```python
import onnxruntime as ort
import numpy as np
import cv2

# 1. åˆ›å»ºONNXä¼šè¯
session = ort.InferenceSession(
    'model.onnx',
    providers=['CUDAExecutionProvider', 'CPUExecutionProvider']
)

# 2. å‡†å¤‡è¾“å…¥
image = cv2.imread('test.jpg')
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
image = cv2.resize(image, (512, 512))

# å½’ä¸€åŒ–
image = image.astype(np.float32) / 255.0
image = (image - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]
image = image.transpose(2, 0, 1)
image = np.expand_dims(image, 0)

# 3. æ¨ç†
input_name = session.get_inputs()[0].name
output_name = session.get_outputs()[0].name

pred = session.run([output_name], {input_name: image})[0]

# 4. åå¤„ç†
pred = 1 / (1 + np.exp(-pred))  # Sigmoid
pred_mask = (pred[0, 0] > 0.5).astype(np.uint8) * 255

# 5. ä¿å­˜
cv2.imwrite('onnx_result.png', pred_mask)
```

## 5. è¯„ä¼°æŒ‡æ ‡è®¡ç®—

```python
import torch
import numpy as np
from sklearn.metrics import precision_recall_fscore_support

def compute_metrics(preds, targets, threshold=0.5):
    """
    è®¡ç®—åˆ†å‰²æŒ‡æ ‡
    
    Args:
        preds: (N, 1, H, W) é¢„æµ‹æ¦‚ç‡
        targets: (N, 1, H, W) çœŸå®æ ‡ç­¾
        threshold: äºŒå€¼åŒ–é˜ˆå€¼
    """
    preds_binary = (preds > threshold).float()
    
    # IoU
    intersection = (preds_binary * targets).sum()
    union = (preds_binary + targets).clamp(0, 1).sum()
    iou = (intersection / (union + 1e-8)).item()
    
    # Dice
    dice = (2 * intersection / (preds_binary.sum() + targets.sum() + 1e-8)).item()
    
    # Precision, Recall, F1
    preds_flat = preds_binary.view(-1).cpu().numpy()
    targets_flat = targets.view(-1).cpu().numpy()
    
    precision, recall, f1, _ = precision_recall_fscore_support(
        targets_flat, preds_flat, average='binary', zero_division=0
    )
    
    return {
        'iou': iou,
        'dice': dice,
        'precision': precision,
        'recall': recall,
        'f1': f1
    }

# ä½¿ç”¨ç¤ºä¾‹
metrics = compute_metrics(pred_probs, gt_masks)
print(f"IoU: {metrics['iou']:.4f}")
print(f"Dice: {metrics['dice']:.4f}")
print(f"Precision: {metrics['precision']:.4f}")
print(f"Recall: {metrics['recall']:.4f}")
print(f"F1: {metrics['f1']:.4f}")
```

## 6. å¯è§†åŒ–ç¤ºä¾‹

```python
import cv2
import numpy as np
import matplotlib.pyplot as plt

def visualize_result(image, gt_mask, pred_mask, save_path=None):
    """
    å¯è§†åŒ–åˆ†å‰²ç»“æœ
    
    Args:
        image: (H, W, 3) RGBå›¾åƒ
        gt_mask: (H, W) çœŸå®æ©ç 
        pred_mask: (H, W) é¢„æµ‹æ©ç 
    """
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    
    # åŸå›¾
    axes[0, 0].imshow(image)
    axes[0, 0].set_title('Original Image')
    axes[0, 0].axis('off')
    
    # çœŸå®æ©ç 
    axes[0, 1].imshow(gt_mask, cmap='gray')
    axes[0, 1].set_title('Ground Truth')
    axes[0, 1].axis('off')
    
    # é¢„æµ‹æ©ç 
    axes[0, 2].imshow(pred_mask, cmap='gray')
    axes[0, 2].set_title('Prediction')
    axes[0, 2].axis('off')
    
    # å åŠ çœŸå®
    overlay_gt = image.copy()
    overlay_gt[gt_mask > 0] = [255, 0, 0]  # çº¢è‰²
    axes[1, 0].imshow(overlay_gt)
    axes[1, 0].set_title('GT Overlay')
    axes[1, 0].axis('off')
    
    # å åŠ é¢„æµ‹
    overlay_pred = image.copy()
    overlay_pred[pred_mask > 0] = [0, 255, 0]  # ç»¿è‰²
    axes[1, 1].imshow(overlay_pred)
    axes[1, 1].set_title('Pred Overlay')
    axes[1, 1].axis('off')
    
    # å¯¹æ¯”ï¼ˆTP=ç»¿, FP=çº¢, FN=è“ï¼‰
    comparison = np.zeros((*gt_mask.shape, 3), dtype=np.uint8)
    tp = (gt_mask > 0) & (pred_mask > 0)
    fp = (gt_mask == 0) & (pred_mask > 0)
    fn = (gt_mask > 0) & (pred_mask == 0)
    
    comparison[tp] = [0, 255, 0]  # TP - ç»¿è‰²
    comparison[fp] = [255, 0, 0]  # FP - çº¢è‰²
    comparison[fn] = [0, 0, 255]  # FN - è“è‰²
    
    axes[1, 2].imshow(comparison)
    axes[1, 2].set_title('TP(Green) FP(Red) FN(Blue)')
    axes[1, 2].axis('off')
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
    else:
        plt.show()
    
    plt.close()

# ä½¿ç”¨
visualize_result(image, gt_mask, pred_mask, save_path='visualization.png')
```

## 7. æ¨¡å‹å¾®è°ƒç¤ºä¾‹

```python
# ä»æ£€æŸ¥ç‚¹å¾®è°ƒæ¨¡å‹
trainer = Trainer(...)
trainer.load_checkpoint('outputs/best.pth')

# å†»ç»“ç¼–ç å™¨ï¼Œåªè®­ç»ƒè§£ç å™¨
for param in trainer.model.encoder.parameters():
    param.requires_grad = False

# é™ä½å­¦ä¹ ç‡
for param_group in trainer.optimizer.param_groups:
    param_group['lr'] = 1e-5

# ç»§ç»­è®­ç»ƒ
trainer.train(train_loader, val_loader, num_epochs=50)
```

---

ä»¥ä¸Šç¤ºä¾‹æ¶µç›–äº†ä»è®­ç»ƒåˆ°éƒ¨ç½²çš„å®Œæ•´æµç¨‹ï¼Œå¯æ ¹æ®å®é™…éœ€æ±‚è°ƒæ•´ï¼

