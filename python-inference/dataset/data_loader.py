"""
é«˜æ€§èƒ½æ•°æ®åŠ è½½æ¨¡å—
æ”¯æŒå¤šç§æ ¼å¼ã€å¤šçº§ç¼“å­˜ã€éš¾ä¾‹æŒ–æ˜
"""

import os
import json
import yaml
import cv2
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Union
from dataclasses import dataclass
import albumentations as A
from albumentations.pytorch import ToTensorV2
import torch
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import lmdb
import pickle


@dataclass
class DatasetConfig:
    """æ•°æ®é›†é…ç½®"""
    root: str
    image_dir: str = "images"
    mask_dir: str = "masks"
    split: str = "train"  # train, val, test
    crop_size: Tuple[int, int] = (512, 512)
    train_scales: List[int] = (256, 384, 512)
    normalize_mean: Tuple[float, float, float] = (0.485, 0.456, 0.406)
    normalize_std: Tuple[float, float, float] = (0.229, 0.224, 0.225)
    use_cache: bool = True
    cache_dir: str = "./cache"


class AnnotationConverter:
    """å¤šæ ¼å¼æ ‡æ³¨è½¬æ¢å™¨"""
    
    @staticmethod
    def coco_to_mask(coco_annotation: Dict, image_shape: Tuple[int, int]) -> np.ndarray:
        """COCOæ ¼å¼ -> äºŒå€¼æ©ç """
        from pycocotools import mask as mask_utils
        
        h, w = image_shape
        mask = np.zeros((h, w), dtype=np.uint8)
        
        for ann in coco_annotation.get('annotations', []):
            if 'segmentation' in ann:
                # Polygonæ ¼å¼
                if isinstance(ann['segmentation'], list):
                    rles = mask_utils.frPyObjects(ann['segmentation'], h, w)
                    rle = mask_utils.merge(rles)
                # RLEæ ¼å¼
                else:
                    rle = ann['segmentation']
                m = mask_utils.decode(rle)
                mask = np.maximum(mask, m)
        
        return (mask > 0).astype(np.uint8) * 255
    
    @staticmethod
    def voc_to_mask(xml_path: str, image_shape: Tuple[int, int]) -> np.ndarray:
        """VOC XML -> äºŒå€¼æ©ç """
        import xml.etree.ElementTree as ET
        
        h, w = image_shape
        mask = np.zeros((h, w), dtype=np.uint8)
        
        tree = ET.parse(xml_path)
        root = tree.getroot()
        
        for obj in root.findall('object'):
            polygon = obj.find('polygon')
            if polygon is not None:
                points = []
                for pt in polygon:
                    x = int(pt.find('x').text)
                    y = int(pt.find('y').text)
                    points.append([x, y])
                
                points = np.array(points, dtype=np.int32)
                cv2.fillPoly(mask, [points], 255)
        
        return mask
    
    @staticmethod
    def yolo_to_mask(txt_path: str, image_shape: Tuple[int, int]) -> np.ndarray:
        """YOLO TXT -> äºŒå€¼æ©ç ï¼ˆæ”¯æŒåˆ†å‰²æ ¼å¼ï¼‰"""
        h, w = image_shape
        mask = np.zeros((h, w), dtype=np.uint8)
        
        with open(txt_path, 'r') as f:
            for line in f:
                parts = line.strip().split()
                if len(parts) < 5:
                    continue
                
                # YOLOåˆ†å‰²æ ¼å¼: class_id x1 y1 x2 y2 ... xn yn
                points = np.array(parts[1:], dtype=np.float32).reshape(-1, 2)
                points[:, 0] *= w
                points[:, 1] *= h
                points = points.astype(np.int32)
                
                cv2.fillPoly(mask, [points], 255)
        
        return mask


class QualityControl:
    """æ•°æ®è´¨é‡æ§åˆ¶"""
    
    @staticmethod
    def check_size_consistency(image_path: str, mask_path: str) -> bool:
        """æ£€æŸ¥å›¾åƒå’Œæ©ç å°ºå¯¸ä¸€è‡´æ€§"""
        img = cv2.imread(image_path)
        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
        
        if img is None or mask is None:
            return False
        
        return img.shape[:2] == mask.shape[:2]
    
    @staticmethod
    def filter_small_artifacts(mask: np.ndarray, 
                              min_area: int = 50,
                              min_aspect_ratio: float = 0.05) -> np.ndarray:
        """è¿‡æ»¤å°é¢ç§¯ä¼ªå½±"""
        # è¿é€šåŸŸåˆ†æ
        num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(
            mask, connectivity=8)
        
        filtered_mask = np.zeros_like(mask)
        
        for i in range(1, num_labels):
            area = stats[i, cv2.CC_STAT_AREA]
            w = stats[i, cv2.CC_STAT_WIDTH]
            h = stats[i, cv2.CC_STAT_HEIGHT]
            
            # è¿‡æ»¤æ¡ä»¶
            if area < min_area:
                continue
            
            aspect_ratio = min(w, h) / max(w, h) if max(w, h) > 0 else 0
            if aspect_ratio < min_aspect_ratio:
                continue
            
            filtered_mask[labels == i] = 255
        
        return filtered_mask
    
    @staticmethod
    def detect_annotation_errors(image: np.ndarray, 
                                 mask: np.ndarray,
                                 threshold: float = 0.8) -> bool:
        """åŸºäºç»Ÿè®¡ç‰¹å¾æ£€æµ‹æ ‡æ³¨é”™è¯¯ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # æ£€æŸ¥æ©ç æ˜¯å¦å…¨é»‘æˆ–å…¨ç™½
        mask_ratio = np.sum(mask > 0) / mask.size
        if mask_ratio < 0.001 or mask_ratio > 0.95:
            return False
        
        # æ£€æŸ¥æ©ç åŒºåŸŸä¸å›¾åƒçš„å¯¹æ¯”åº¦
        crack_region = image[mask > 0]
        bg_region = image[mask == 0]
        
        if len(crack_region) > 0 and len(bg_region) > 0:
            crack_mean = np.mean(crack_region)
            bg_mean = np.mean(bg_region)
            contrast = abs(crack_mean - bg_mean) / 255.0
            
            # è£‚çº¹åº”è¯¥ä¸èƒŒæ™¯æœ‰æ˜æ˜¾å¯¹æ¯”
            if contrast < 0.1:
                return False
        
        return True


class CrackDataset(Dataset):
    """è£‚çº¹åˆ†å‰²æ•°æ®é›†"""
    
    def __init__(self, 
                 config: DatasetConfig,
                 transform: Optional[A.Compose] = None,
                 use_hard_mining: bool = False):
        """
        Args:
            config: æ•°æ®é›†é…ç½®
            transform: Albumentationså˜æ¢
            use_hard_mining: æ˜¯å¦ä½¿ç”¨éš¾ä¾‹æŒ–æ˜
        """
        self.config = config
        self.transform = transform
        self.use_hard_mining = use_hard_mining
        
        # åŠ è½½æ•°æ®é›†ç´¢å¼•
        self.samples = self._load_samples()
        
        # éš¾ä¾‹æƒé‡ï¼ˆåˆå§‹å‡åŒ€ï¼‰
        self.sample_weights = np.ones(len(self.samples))
        
        # LMDBç¼“å­˜
        self.lmdb_env = None
        if config.use_cache:
            self._init_cache()
    
    def _load_samples(self) -> List[Dict]:
        """åŠ è½½æ ·æœ¬åˆ—è¡¨"""
        split_file = Path(self.config.root) / f"{self.config.split}.txt"
        
        print(f"\n{'='*60}")
        print(f"ğŸ” è°ƒè¯•ä¿¡æ¯ - åŠ è½½ {self.config.split} æ•°æ®é›†")
        print(f"{'='*60}")
        print(f"ğŸ“‚ Rootè·¯å¾„: {self.config.root}")
        print(f"ğŸ“‚ Rootç»å¯¹è·¯å¾„: {Path(self.config.root).resolve()}")
        print(f"ğŸ“„ Splitæ–‡ä»¶: {split_file}")
        print(f"ğŸ“„ Splitæ–‡ä»¶å­˜åœ¨: {split_file.exists()}")
        
        if not split_file.exists():
            print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ° split æ–‡ä»¶!")
            print(f"   è¯·æ£€æŸ¥è·¯å¾„: {split_file.resolve()}")
            return []
        
        samples = []
        checked_count = 0
        failed_checks = {
            'image_not_found': 0,
            'mask_not_found': 0,
            'size_mismatch': 0
        }
        
        with open(split_file, 'r') as f:
            lines = f.readlines()
            total = len(lines)
            print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {total}")
            
            for idx, line in enumerate(lines):
                sample_id = line.strip()
                if not sample_id:  # è·³è¿‡ç©ºè¡Œ
                    continue
                
                # åªæ˜¾ç¤ºå‰3ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
                if idx < 3:
                    print(f"\n--- æ ·æœ¬ {idx+1}: {sample_id} ---")
                
                image_path = Path(self.config.root) / self.config.image_dir / self.config.split / f"{sample_id}.jpg"
                mask_path = Path(self.config.root) / self.config.mask_dir / self.config.split / f"{sample_id}.png"
                
                if idx < 3:
                    print(f"  å›¾åƒè·¯å¾„: {image_path}")
                    print(f"  å›¾åƒå­˜åœ¨: {image_path.exists()}")
                    print(f"  æ©ç è·¯å¾„: {mask_path}")
                    print(f"  æ©ç å­˜åœ¨: {mask_path.exists()}")
                
                if not image_path.exists():
                    failed_checks['image_not_found'] += 1
                    continue
                
                if not mask_path.exists():
                    failed_checks['mask_not_found'] += 1
                    continue
                
                # è´¨é‡æ£€æŸ¥ï¼ˆåªæ£€æŸ¥å‰100ä¸ªæ ·æœ¬ä»¥åŠ å¿«é€Ÿåº¦ï¼‰
                checked_count += 1
                if checked_count <= 100:
                    if QualityControl.check_size_consistency(
                        str(image_path), str(mask_path)):
                        samples.append({
                            'id': sample_id,
                            'image': str(image_path),
                            'mask': str(mask_path)
                        })
                        if idx < 3:
                            print(f"  âœ… é€šè¿‡è´¨é‡æ£€æŸ¥")
                    else:
                        failed_checks['size_mismatch'] += 1
                        if idx < 3:
                            print(f"  âŒ è´¨é‡æ£€æŸ¥å¤±è´¥ï¼ˆå°ºå¯¸ä¸åŒ¹é…ï¼‰")
                else:
                    # åç»­æ ·æœ¬è·³è¿‡è´¨é‡æ£€æŸ¥ä»¥åŠ å¿«é€Ÿåº¦
                    samples.append({
                        'id': sample_id,
                        'image': str(image_path),
                        'mask': str(mask_path)
                    })
                
                # è¿›åº¦æ˜¾ç¤º
                if (idx + 1) % 500 == 0:
                    print(f"  è¿›åº¦: {idx+1}/{total} ({(idx+1)*100//total}%) - å·²åŠ è½½ {len(samples)} ä¸ªæœ‰æ•ˆæ ·æœ¬")
        
        print(f"\n{'='*60}")
        print(f"ğŸ“Š åŠ è½½ç»Ÿè®¡:")
        print(f"  - æ€»æ ·æœ¬æ•°: {total}")
        print(f"  - å›¾åƒæœªæ‰¾åˆ°: {failed_checks['image_not_found']}")
        print(f"  - æ©ç æœªæ‰¾åˆ°: {failed_checks['mask_not_found']}")
        print(f"  - å°ºå¯¸ä¸åŒ¹é…: {failed_checks['size_mismatch']}")
        print(f"  - æˆåŠŸåŠ è½½: {len(samples)}")
        print(f"{'='*60}\n")
        
        if len(samples) == 0:
            print(f"âš ï¸  è­¦å‘Š: æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•æ ·æœ¬!")
            print(f"   è¯·æ£€æŸ¥:")
            print(f"   1. æ•°æ®é›†è·¯å¾„æ˜¯å¦æ­£ç¡®")
            print(f"   2. å›¾åƒå’Œæ©ç æ–‡ä»¶æ˜¯å¦å­˜åœ¨")
            print(f"   3. æ–‡ä»¶æ‰©å±•åæ˜¯å¦ä¸º .jpg å’Œ .png")
        
        return samples
    
    def _init_cache(self):
        """åˆå§‹åŒ–LMDBç¼“å­˜"""
        cache_path = Path(self.config.cache_dir) / self.config.split
        cache_path.mkdir(parents=True, exist_ok=True)
        
        self.lmdb_env = lmdb.open(
            str(cache_path),
            map_size=5 * 1024 * 1024 * 1024,  # 10GB
            readonly=False,
            lock=False
        )
    
    def _get_from_cache(self, idx: int) -> Optional[Tuple[np.ndarray, np.ndarray]]:
        """ä»ç¼“å­˜è¯»å–"""
        if self.lmdb_env is None:
            return None
        
        with self.lmdb_env.begin() as txn:
            data = txn.get(str(idx).encode())
            if data is not None:
                return pickle.loads(data)
        return None
    
    def _put_to_cache(self, idx: int, image: np.ndarray, mask: np.ndarray):
        """å†™å…¥ç¼“å­˜"""
        if self.lmdb_env is None:
            return
        
        with self.lmdb_env.begin(write=True) as txn:
            txn.put(str(idx).encode(), pickle.dumps((image, mask)))
    
    def __len__(self) -> int:
        return len(self.samples)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        """è·å–æ ·æœ¬"""
        # å°è¯•ä»ç¼“å­˜åŠ è½½
        cached = self._get_from_cache(idx)
        if cached is not None:
            image, mask = cached
        else:
            # ä»ç£ç›˜åŠ è½½
            sample = self.samples[idx]
            image = cv2.imread(sample['image'])
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            mask = cv2.imread(sample['mask'], cv2.IMREAD_GRAYSCALE)
            
            # è´¨é‡æ§åˆ¶ï¼šè¿‡æ»¤å°ä¼ªå½±
            mask = QualityControl.filter_small_artifacts(mask)
            
            # å†™å…¥ç¼“å­˜
            self._put_to_cache(idx, image, mask)
        
        # æ•°æ®å¢å¼º
        if self.transform is not None:
            transformed = self.transform(image=image, mask=mask)
            image = transformed['image']
            mask = transformed['mask']
            
            # å½’ä¸€åŒ–maskåˆ°0/1ï¼ˆå·²ç»æ˜¯Tensorï¼‰
            mask = (mask > 0.5).float()
            # æ·»åŠ  channel ç»´åº¦: [H, W] -> [1, H, W]
            mask = mask.unsqueeze(0)
        else:
            # å¦‚æœæ²¡æœ‰transformï¼Œæ‰‹åŠ¨å¤„ç†
            mask = (mask > 128).astype(np.float32)
            # æ·»åŠ  channel ç»´åº¦
            mask = np.expand_dims(mask, axis=0)
        
        return {
            'image': image,
            'mask': mask,
            'id': self.samples[idx]['id'],
            'weight': self.sample_weights[idx]
        }
    
    def update_sample_weights(self, losses: np.ndarray):
        """æ›´æ–°éš¾ä¾‹æƒé‡ï¼ˆHard Example Miningï¼‰"""
        if not self.use_hard_mining:
            return
        
        # åŸºäºæŸå¤±å€¼æ›´æ–°æƒé‡
        self.sample_weights = losses / (losses.mean() + 1e-8)
        self.sample_weights = np.clip(self.sample_weights, 0.5, 2.0)


def get_training_augmentation(config: DatasetConfig, epoch_ratio: float = 0.0) -> A.Compose:
    """
    è·å–è®­ç»ƒå¢å¼ºç­–ç•¥
    epoch_ratio: è®­ç»ƒè¿›åº¦ 0.0~1.0ï¼Œç”¨äºåŠ¨æ€è°ƒæ•´å¢å¼ºå¼ºåº¦
    """
    # åŠ¨æ€è°ƒæ•´å¢å¼ºå¼ºåº¦ï¼ˆå‰60% epochå¼ºå¢å¼ºï¼Œå40%å¼±å¢å¼ºï¼‰
    strong_aug = epoch_ratio < 0.6
    
    transforms = []
    
    # å‡ ä½•å¢å¼º
    if strong_aug:
        transforms.extend([
            A.RandomScale(scale_limit=(-0.5, 1.0), p=0.5),
            A.Rotate(limit=90, p=0.5),
            A.Affine(scale=(0.8, 1.2), translate_percent=0.1, p=0.3),
            A.ElasticTransform(alpha=50, sigma=5, p=0.3),  # ç§»é™¤ alpha_affine
        ])
    else:
        transforms.extend([
            A.RandomScale(scale_limit=(-0.2, 0.2), p=0.3),
            A.Rotate(limit=30, p=0.3),
        ])
    
    # ç¿»è½¬
    transforms.extend([
        A.HorizontalFlip(p=0.5),
        A.VerticalFlip(p=0.3),
    ])
    
    # é¢œè‰²å¢å¼º
    if strong_aug:
        transforms.extend([
            A.CLAHE(clip_limit=4.0, p=0.5),
            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),
            A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),
            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.3),
        ])
    else:
        transforms.extend([
            A.CLAHE(clip_limit=2.0, p=0.3),
            A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.3),
        ])
    
    # å™ªå£°ä¸æ¨¡ç³Š
    transforms.extend([
        A.OneOf([
            A.GaussNoise(p=1.0),  # ä½¿ç”¨é»˜è®¤å‚æ•°
            A.ISONoise(color_shift=(0.01, 0.05), intensity=(0.1, 0.5), p=1.0),
        ], p=0.3),
        A.OneOf([
            A.MotionBlur(blur_limit=7, p=1.0),
            A.MedianBlur(blur_limit=7, p=1.0),
            A.GaussianBlur(blur_limit=7, p=1.0),
        ], p=0.2),
    ])
    
    # è£å‰ªï¼ˆå¤šå°ºåº¦è®­ç»ƒï¼‰
    if strong_aug and len(config.train_scales) > 1:
        crop_size = np.random.choice(config.train_scales)
    else:
        crop_size = config.crop_size[0]
    
    # å…ˆç¡®ä¿å›¾åƒå¤§å°è¶³å¤Ÿè¿›è¡Œè£å‰ª
    # ä½¿ç”¨ LongestMaxSize + PadIfNeeded ç¡®ä¿å›¾åƒä¸ä¼šå¤ªå°
    transforms.extend([
        A.LongestMaxSize(max_size=max(crop_size * 2, 1024), p=1.0),  # ç¡®ä¿å›¾åƒè¶³å¤Ÿå¤§
        A.PadIfNeeded(
            min_height=crop_size,
            min_width=crop_size,
            border_mode=cv2.BORDER_REFLECT_101,
            p=1.0
        ),
        A.RandomCrop(height=crop_size, width=crop_size, p=1.0)
    ])
    
    # å½’ä¸€åŒ–ä¸è½¬æ¢
    transforms.extend([
        A.Normalize(mean=config.normalize_mean, std=config.normalize_std),
        ToTensorV2(),
    ])
    
    return A.Compose(transforms)


def get_validation_augmentation(config: DatasetConfig) -> A.Compose:
    """éªŒè¯é›†å¢å¼ºï¼ˆä»…å½’ä¸€åŒ–ï¼‰"""
    return A.Compose([
        A.Resize(config.crop_size[0], config.crop_size[1]),
        A.Normalize(mean=config.normalize_mean, std=config.normalize_std),
        ToTensorV2(),
    ])


def create_dataloaders(config: DatasetConfig,
                       batch_size: int = 16,
                       num_workers: int = 4,
                       epoch_ratio: float = 0.0) -> Tuple[DataLoader, DataLoader]:
    """åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨"""
    
    # è®­ç»ƒé›†
    train_config = DatasetConfig(**{**config.__dict__, 'split': 'train'})
    train_dataset = CrackDataset(
        train_config,
        transform=get_training_augmentation(train_config, epoch_ratio),
        use_hard_mining=True
    )
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        prefetch_factor=2 if num_workers > 0 else None,  # åªåœ¨å¤šè¿›ç¨‹æ—¶è®¾ç½®
        persistent_workers=True if num_workers > 0 else False
    )
    
    # éªŒè¯é›†
    val_config = DatasetConfig(**{**config.__dict__, 'split': 'val'})
    val_dataset = CrackDataset(
        val_config,
        transform=get_validation_augmentation(val_config),
        use_hard_mining=False
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        prefetch_factor=2 if num_workers > 0 else None,  # åªåœ¨å¤šè¿›ç¨‹æ—¶è®¾ç½®
        persistent_workers=True if num_workers > 0 else False
    )
    
    return train_loader, val_loader


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    config = DatasetConfig(
        root="./data/processed",
        crop_size=(512, 512),
        train_scales=[256, 384, 512]
    )
    
    train_loader, val_loader = create_dataloaders(config, batch_size=4, num_workers=2)
    
    for batch in train_loader:
        print(f"Image shape: {batch['image'].shape}")
        print(f"Mask shape: {batch['mask'].shape}")
        print(f"Sample weights: {batch['weight']}")
        break

