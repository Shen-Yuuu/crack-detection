"""
çµæ´»æ•°æ®é›†å‡†å¤‡å·¥å…·

æ”¯æŒé€‰æ‹©å•ä¸ªæˆ–å¤šä¸ªæ•°æ®é›†è¿›è¡Œè®­ç»ƒ

ä½¿ç”¨æ–¹æ³•:
    # æŸ¥çœ‹æ‰€æœ‰æ•°æ®é›†ä¿¡æ¯
    python dataset_selector.py --info
    
    # å‡†å¤‡å•ä¸ªæ•°æ®é›† (ä¾‹å¦‚ CrackTree260ï¼Œå¸¸ç”¨äºè®ºæ–‡å¯¹æ¯”)
    python dataset_selector.py --datasets CrackTree260 --output ../data/cracktree260
    
    # å‡†å¤‡å¤šä¸ªæ•°æ®é›†
    python dataset_selector.py --datasets Crack500 CFD CrackTree260 --output ../data/mixed
    
    # å‡†å¤‡æ‰€æœ‰æ•°æ®é›†
    python dataset_selector.py --datasets all --output ../data/all

æ”¯æŒçš„æ•°æ®é›†:
    - AsphaltCrack300: æ²¥é’è·¯é¢è£‚ç¼ (~300å¼ )
    - CFD: æ··å‡åœŸè£‚ç¼ (~118å¼ )
    - Crack500: è·¯é¢è£‚ç¼ (~500å¼ )
    - CrackTree260: æ ‘çŠ¶è£‚ç¼ (~260å¼ ) [DeepCrackè®ºæ–‡å¸¸ç”¨]
    - CrackLS315: å¤šè¡¨é¢è£‚ç¼ (~315å¼ )
    - CRKWH100: æ­¦æ±‰é“è·¯è£‚ç¼ (~100å¼ )
"""

import os
import sys
import io
import cv2
import numpy as np
from pathlib import Path
import shutil
from tqdm import tqdm
import random
import json
from typing import List, Tuple, Dict, Optional
import argparse

# è®¾ç½®æ ‡å‡†è¾“å‡ºç¼–ç ä¸ºUTF-8
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')


# ============== æ•°æ®é›†é…ç½® ==============
DATASET_REGISTRY = {
    'AsphaltCrack300': {
        'description': 'æ²¥é’è·¯é¢è£‚ç¼æ•°æ®é›†',
        'source': 'CrackDataset',
        'base_path': 'CrackDataset-main/AsphaltCrack300',
        'train_images': 'train',
        'train_masks': 'label',
        'val_images': None,
        'val_masks': None,
        'image_ext': '.jpg',
        'mask_ext': '.png',
        'expected_count': 300,
    },
    'CFD': {
        'description': 'æ··å‡åœŸè£‚ç¼æ•°æ®é›† (Concrete Fracture Dataset)',
        'source': 'CrackDataset',
        'base_path': 'CrackDataset-main/CFD',
        'train_images': 'train',
        'train_masks': 'label',
        'val_images': 'val',
        'val_masks': 'val_label',
        'image_ext': '.jpg',
        'mask_ext': '.png',
        'expected_count': 118,
    },
    'Crack500': {
        'description': 'è·¯é¢è£‚ç¼æ•°æ®é›†',
        'source': 'CrackDataset',
        'base_path': 'CrackDataset-main/crack500',
        'train_images': 'train',
        'train_masks': 'label',
        'val_images': 'val',
        'val_masks': 'val_label',
        'image_ext': '.jpg',
        'mask_ext': '.png',
        'expected_count': 500,
    },
    'CrackTree260': {
        'description': 'æ ‘çŠ¶è£‚ç¼æ•°æ®é›† (DeepCrackè®ºæ–‡å¸¸ç”¨)',
        'source': 'DeepCrack',
        'base_path': 'DeepCrack-datasets',
        'train_images': 'CrackTree260',
        'train_masks': 'CrackTree260_gt/gt',
        'val_images': None,
        'val_masks': None,
        'image_ext': ['.jpg', '.JPG'],
        'mask_ext': '.png',
        'expected_count': 260,
        'mask_in_subdir': True,
    },
    'CrackLS315': {
        'description': 'å¤šè¡¨é¢è£‚ç¼æ•°æ®é›†',
        'source': 'DeepCrack',
        'base_path': 'DeepCrack-datasets',
        'train_images': 'CrackLS315',
        'train_masks': 'CrackLS315_gt',
        'val_images': None,
        'val_masks': None,
        'image_ext': '.jpg',
        'mask_ext': '.png',
        'expected_count': 315,
    },
    'CRKWH100': {
        'description': 'æ­¦æ±‰é“è·¯è£‚ç¼æ•°æ®é›†',
        'source': 'DeepCrack',
        'base_path': 'DeepCrack-datasets',
        'train_images': 'CRKWH100',
        'train_masks': 'CRKWH100_gt',
        'val_images': None,
        'val_masks': None,
        'image_ext': '.png',
        'mask_ext': '.png',
        'expected_count': 100,
    },
}


class DatasetSelector:
    """çµæ´»æ•°æ®é›†é€‰æ‹©å™¨"""
    
    def __init__(self, datasets_root: str):
        self.datasets_root = Path(datasets_root)
    
    def get_available_datasets(self) -> List[str]:
        """è·å–å¯ç”¨çš„æ•°æ®é›†åˆ—è¡¨"""
        available = []
        for name, config in DATASET_REGISTRY.items():
            base = self.datasets_root / config['base_path']
            img_dir = base / config['train_images'] if config['train_images'] else base
            if img_dir.exists():
                available.append(name)
        return available
    
    def count_images(self, dataset_name: str) -> Dict[str, int]:
        """ç»Ÿè®¡æ•°æ®é›†å›¾åƒæ•°é‡"""
        if dataset_name not in DATASET_REGISTRY:
            return {'train': 0, 'val': 0, 'total': 0}
        
        config = DATASET_REGISTRY[dataset_name]
        base = self.datasets_root / config['base_path']
        
        train_count = 0
        val_count = 0
        
        # è®­ç»ƒé›†
        img_dir = base / config['train_images']
        if img_dir.exists():
            exts = config['image_ext'] if isinstance(config['image_ext'], list) else [config['image_ext']]
            for ext in exts:
                train_count += len(list(img_dir.glob(f'*{ext}')))
        
        # éªŒè¯é›†
        if config['val_images']:
            val_dir = base / config['val_images']
            if val_dir.exists():
                exts = config['image_ext'] if isinstance(config['image_ext'], list) else [config['image_ext']]
                for ext in exts:
                    val_count += len(list(val_dir.glob(f'*{ext}')))
        
        return {
            'train': train_count,
            'val': val_count,
            'total': train_count + val_count
        }
    
    def print_info(self):
        """æ‰“å°æ‰€æœ‰æ•°æ®é›†ä¿¡æ¯"""
        print("\n" + "=" * 70)
        print("ğŸ“¦ è£‚çº¹æ£€æµ‹æ•°æ®é›†æ¦‚è§ˆ")
        print("=" * 70)
        
        available = self.get_available_datasets()
        total_all = 0
        
        for name, config in DATASET_REGISTRY.items():
            status = "âœ…" if name in available else "âŒ"
            counts = self.count_images(name)
            
            print(f"\n{status} {name}")
            print(f"   ğŸ“ {config['description']}")
            print(f"   ğŸ“ æ¥æº: {config['source']}")
            print(f"   ğŸ–¼ï¸  æ•°é‡: {counts['total']} å¼ ", end="")
            if counts['val'] > 0:
                print(f" (è®­ç»ƒ: {counts['train']}, éªŒè¯: {counts['val']})")
            else:
                print()
            
            if name in available:
                total_all += counts['total']
        
        print("\n" + "-" * 70)
        print(f"ğŸ“Š å¯ç”¨æ•°æ®é›†æ€»è®¡: {len(available)} ä¸ª, {total_all} å¼ å›¾åƒ")
        print("=" * 70)
        
        print("\nğŸ’¡ ä½¿ç”¨ç¤ºä¾‹:")
        print("   # å•ä¸ªæ•°æ®é›† (é€‚åˆè®ºæ–‡å¯¹æ¯”)")
        print("   python dataset_selector.py --datasets CrackTree260 --output ../data/cracktree260")
        print("\n   # å¤šä¸ªæ•°æ®é›†")
        print("   python dataset_selector.py --datasets Crack500 CFD --output ../data/crack_cfd")
        print("\n   # æ‰€æœ‰æ•°æ®é›†")
        print("   python dataset_selector.py --datasets all --output ../data/all\n")
    
    def load_dataset_pairs(self, dataset_name: str) -> List[Tuple[Path, Path, str]]:
        """åŠ è½½æ•°æ®é›†çš„å›¾åƒ-maskå¯¹"""
        if dataset_name not in DATASET_REGISTRY:
            raise ValueError(f"æœªçŸ¥æ•°æ®é›†: {dataset_name}")
        
        config = DATASET_REGISTRY[dataset_name]
        base = self.datasets_root / config['base_path']
        
        pairs = []
        
        def find_mask(img_path: Path, mask_dir: Path, mask_ext: str) -> Optional[Path]:
            """æŸ¥æ‰¾å¯¹åº”çš„maskæ–‡ä»¶"""
            stem = img_path.stem
            
            # å°è¯•å¤šç§å¯èƒ½çš„maskå‘½å
            candidates = [
                mask_dir / f"{stem}{mask_ext}",
                mask_dir / f"{stem}.png",
                mask_dir / f"{stem}.bmp",
                mask_dir / f"{stem}_mask{mask_ext}",
            ]
            
            for c in candidates:
                if c.exists():
                    return c
            return None
        
        def load_from_dir(img_dir: Path, mask_dir: Path, split_name: str):
            """ä»ç›®å½•åŠ è½½å›¾åƒå¯¹"""
            if not img_dir.exists():
                return
            
            exts = config['image_ext'] if isinstance(config['image_ext'], list) else [config['image_ext']]
            mask_ext = config['mask_ext']
            
            for ext in exts:
                for img_path in img_dir.glob(f'*{ext}'):
                    mask_path = find_mask(img_path, mask_dir, mask_ext)
                    if mask_path:
                        pairs.append((img_path, mask_path, split_name))
        
        # åŠ è½½è®­ç»ƒé›†
        train_img_dir = base / config['train_images']
        train_mask_dir = base / config['train_masks']
        load_from_dir(train_img_dir, train_mask_dir, 'train')
        
        # åŠ è½½éªŒè¯é›†ï¼ˆå¦‚æœæœ‰ï¼‰
        if config['val_images'] and config['val_masks']:
            val_img_dir = base / config['val_images']
            val_mask_dir = base / config['val_masks']
            load_from_dir(val_img_dir, val_mask_dir, 'val')
        
        return pairs
    
    def prepare(self,
                dataset_names: List[str],
                output_dir: str,
                train_ratio: float = 0.7,
                val_ratio: float = 0.15,
                test_ratio: float = 0.15,
                target_size: Optional[Tuple[int, int]] = None,
                seed: int = 42):
        """
        å‡†å¤‡é€‰å®šçš„æ•°æ®é›†
        
        Args:
            dataset_names: æ•°æ®é›†åç§°åˆ—è¡¨ï¼Œæˆ– ['all']
            output_dir: è¾“å‡ºç›®å½•
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
            val_ratio: éªŒè¯é›†æ¯”ä¾‹
            test_ratio: æµ‹è¯•é›†æ¯”ä¾‹
            target_size: ç›®æ ‡å°ºå¯¸ (H, W)
            seed: éšæœºç§å­
        """
        random.seed(seed)
        np.random.seed(seed)
        
        output_path = Path(output_dir)
        
        # å¤„ç† 'all'
        if 'all' in dataset_names:
            dataset_names = self.get_available_datasets()
        
        print(f"\n{'='*60}")
        print(f"ğŸ“¦ å‡†å¤‡æ•°æ®é›†: {', '.join(dataset_names)}")
        print(f"{'='*60}")
        
        # æ”¶é›†æ‰€æœ‰æ ·æœ¬
        all_pairs = []
        for name in dataset_names:
            print(f"\nåŠ è½½ {name}...")
            pairs = self.load_dataset_pairs(name)
            # æ·»åŠ æ•°æ®é›†åç§°æ ‡è®°
            pairs_with_source = [(p[0], p[1], name) for p in pairs]
            all_pairs.extend(pairs_with_source)
            print(f"   æ‰¾åˆ° {len(pairs)} ä¸ªæ ·æœ¬")
        
        if not all_pairs:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆæ ·æœ¬!")
            return
        
        print(f"\nğŸ“Š æ€»è®¡: {len(all_pairs)} ä¸ªæ ·æœ¬")
        
        # æ‰“ä¹±
        random.shuffle(all_pairs)
        
        # åˆ’åˆ†
        n = len(all_pairs)
        n_train = int(n * train_ratio)
        n_val = int(n * val_ratio)
        
        train_pairs = all_pairs[:n_train]
        val_pairs = all_pairs[n_train:n_train + n_val]
        test_pairs = all_pairs[n_train + n_val:]
        
        splits = {
            'train': train_pairs,
            'val': val_pairs,
            'test': test_pairs,
        }
        
        # åˆ›å»ºç›®å½•
        for split in ['train', 'val', 'test']:
            (output_path / 'images' / split).mkdir(parents=True, exist_ok=True)
            (output_path / 'masks' / split).mkdir(parents=True, exist_ok=True)
        
        # å¤åˆ¶æ–‡ä»¶
        stats = {}
        file_lists = {}
        
        for split, pairs in splits.items():
            print(f"\nå¤„ç† {split} é›† ({len(pairs)} æ ·æœ¬)...")
            stats[split] = 0
            file_lists[split] = []
            
            for idx, (img_path, mask_path, source) in enumerate(tqdm(pairs, desc=f"  {split}")):
                try:
                    # è¯»å–
                    img = cv2.imread(str(img_path))
                    mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)
                    
                    if img is None or mask is None:
                        continue
                    
                    # å°ºå¯¸æ£€æŸ¥
                    if img.shape[:2] != mask.shape[:2]:
                        mask = cv2.resize(mask, (img.shape[1], img.shape[0]),
                                         interpolation=cv2.INTER_NEAREST)
                    
                    # è°ƒæ•´å°ºå¯¸
                    if target_size:
                        img = cv2.resize(img, (target_size[1], target_size[0]),
                                        interpolation=cv2.INTER_LINEAR)
                        mask = cv2.resize(mask, (target_size[1], target_size[0]),
                                         interpolation=cv2.INTER_NEAREST)
                    
                    # äºŒå€¼åŒ–mask
                    mask = ((mask > 127) * 255).astype(np.uint8)
                    
                    # ä¿å­˜
                    name = f"{source}_{idx:05d}"
                    cv2.imwrite(str(output_path / 'images' / split / f"{name}.png"), img)
                    cv2.imwrite(str(output_path / 'masks' / split / f"{name}.png"), mask)
                    
                    file_lists[split].append(name)
                    stats[split] += 1
                    
                except Exception as e:
                    print(f"   âš ï¸ è·³è¿‡ {img_path}: {e}")
        
        # ä¿å­˜æ–‡ä»¶åˆ—è¡¨
        for split, names in file_lists.items():
            with open(output_path / f"{split}.txt", 'w') as f:
                for name in names:
                    f.write(f"{name}\n")
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        info = {
            'datasets_used': dataset_names,
            'total_samples': sum(stats.values()),
            'splits': stats,
            'split_ratios': {
                'train': train_ratio,
                'val': val_ratio,
                'test': test_ratio,
            },
            'target_size': target_size,
            'seed': seed,
        }
        
        with open(output_path / 'dataset_stats.json', 'w', encoding='utf-8') as f:
            json.dump(info, f, indent=2, ensure_ascii=False)
        
        # æ‰“å°ç»“æœ
        print(f"\n{'='*60}")
        print("âœ… æ•°æ®é›†å‡†å¤‡å®Œæˆ!")
        print(f"{'='*60}")
        print(f"   ä½¿ç”¨æ•°æ®é›†: {', '.join(dataset_names)}")
        print(f"   è¾“å‡ºç›®å½•: {output_path}")
        print(f"   è®­ç»ƒé›†: {stats['train']} æ ·æœ¬")
        print(f"   éªŒè¯é›†: {stats['val']} æ ·æœ¬")
        print(f"   æµ‹è¯•é›†: {stats['test']} æ ·æœ¬")
        print(f"   æ€»è®¡: {sum(stats.values())} æ ·æœ¬")
        print(f"{'='*60}\n")
        
        return info


def main():
    parser = argparse.ArgumentParser(
        description='çµæ´»æ•°æ®é›†å‡†å¤‡å·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # æŸ¥çœ‹æ•°æ®é›†ä¿¡æ¯
  python dataset_selector.py --info
  
  # å‡†å¤‡å•ä¸ªæ•°æ®é›† (CrackTree260ï¼Œè®ºæ–‡å¸¸ç”¨)
  python dataset_selector.py --datasets CrackTree260 --output ../data/cracktree260
  
  # å‡†å¤‡å¤šä¸ªæ•°æ®é›†
  python dataset_selector.py --datasets Crack500 CFD CrackTree260 --output ../data/mixed
  
  # å‡†å¤‡æ‰€æœ‰æ•°æ®é›†
  python dataset_selector.py --datasets all --output ../data/all
        """
    )
    
    parser.add_argument('--datasets-root', type=str, default='../../datasets',
                       help='æ•°æ®é›†æ ¹ç›®å½•')
    parser.add_argument('--info', action='store_true',
                       help='æ˜¾ç¤ºæ‰€æœ‰æ•°æ®é›†ä¿¡æ¯')
    parser.add_argument('--datasets', type=str, nargs='+',
                       help='è¦ä½¿ç”¨çš„æ•°æ®é›†: all, AsphaltCrack300, CFD, Crack500, CrackTree260, CrackLS315, CRKWH100')
    parser.add_argument('--output', type=str, default='../data/processed',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--train-ratio', type=float, default=0.7)
    parser.add_argument('--val-ratio', type=float, default=0.15)
    parser.add_argument('--test-ratio', type=float, default=0.15)
    parser.add_argument('--size', type=int, nargs=2, default=None,
                       help='ç›®æ ‡å°ºå¯¸ H W')
    parser.add_argument('--seed', type=int, default=42)
    
    args = parser.parse_args()
    
    script_dir = Path(__file__).parent
    datasets_root = (script_dir / args.datasets_root).resolve()
    
    selector = DatasetSelector(str(datasets_root))
    
    if args.info:
        selector.print_info()
        return
    
    if not args.datasets:
        print("è¯·æŒ‡å®šæ•°æ®é›†ï¼Œæˆ–ä½¿ç”¨ --info æŸ¥çœ‹å¯ç”¨æ•°æ®é›†")
        print("ç¤ºä¾‹: python dataset_selector.py --datasets CrackTree260 --output ../data/cracktree260")
        return
    
    output_dir = (script_dir / args.output).resolve()
    target_size = tuple(args.size) if args.size else None
    
    selector.prepare(
        dataset_names=args.datasets,
        output_dir=str(output_dir),
        train_ratio=args.train_ratio,
        val_ratio=args.val_ratio,
        test_ratio=args.test_ratio,
        target_size=target_size,
        seed=args.seed,
    )


if __name__ == "__main__":
    main()
