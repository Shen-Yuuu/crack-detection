# ğŸš€ è£‚çº¹æ£€æµ‹ - æ•°æ®å¤„ç†ä¸æ¨¡å‹è®­ç»ƒæ¨¡å—

åŸºäº **ConvNeXt + UPerNet** çš„é«˜æ€§èƒ½é“è·¯è£‚çº¹åˆ†å‰²ç³»ç»Ÿï¼Œé‡‡ç”¨æœ€å‰æ²¿çš„æ·±åº¦å­¦ä¹ æŠ€æœ¯ã€‚

## âœ¨ æ ¸å¿ƒç‰¹æ€§

### ğŸ“Š æ•°æ®å¤„ç†
- âœ… å¤šæ ¼å¼æ”¯æŒï¼ˆCOCO/VOC/YOLO â†’ PNG Maskï¼‰
- âœ… è‡ªåŠ¨è´¨é‡æ§åˆ¶ï¼ˆå°ºå¯¸æ£€æŸ¥ã€å°ä¼ªå½±è¿‡æ»¤ã€æ ‡æ³¨é”™è¯¯æ£€æµ‹ï¼‰
- âœ… éš¾ä¾‹æŒ–æ˜ï¼ˆHard Example Miningï¼‰
- âœ… LMDBç¼“å­˜åŠ é€Ÿ
- âœ… é«˜çº§æ•°æ®å¢å¼ºï¼ˆAlbumentationsï¼‰
  - å‡ ä½•å¢å¼ºï¼šRandomScaleã€Rotateã€Affineã€ElasticTransform
  - é¢œè‰²å¢å¼ºï¼šCLAHEã€RandomBrightnessContrastã€HueSaturationValue
  - å™ªå£°/å¤©æ°”ï¼šGaussNoiseã€MotionBlurã€Rain/Snow/Fog
  - Copy-Pasteï¼ˆç»†è£‚çº¹å¢å¼ºï¼‰

### ğŸ¯ SOTAæ¨¡å‹æ¶æ„
- âœ… **Backbone**: ConvNeXt-T/Sï¼ˆImageNeté¢„è®­ç»ƒï¼‰
- âœ… **Decoder**: UPerNetï¼ˆé‡‘å­—å¡”æ± åŒ– + FPNï¼‰
- âœ… **æ³¨æ„åŠ›æœºåˆ¶**: CBAMï¼ˆé€šé“+ç©ºé—´æ³¨æ„åŠ›ï¼‰
- âœ… **ç»†é•¿ç›®æ ‡ä¼˜åŒ–**: Strip Pooling
- âœ… **è¾¹ç•Œå¢å¼º**: Edge Detection Branchï¼ˆSobelå¼•å¯¼ï¼‰
- âœ… **æ·±åº¦ç›‘ç£**: å¤šå°ºåº¦è¾…åŠ©æŸå¤±

### ğŸ”¥ é«˜çº§æŸå¤±å‡½æ•°
- âœ… Dice Lossï¼ˆåŒºåŸŸé‡å ä¼˜åŒ–ï¼‰
- âœ… Focal Lossï¼ˆç±»åˆ«ä¸å¹³è¡¡å¤„ç†ï¼‰
- âœ… Tversky Lossï¼ˆFP/FNæƒé‡å¯è°ƒï¼‰
- âœ… Boundary Lossï¼ˆè¾¹ç•Œæ•æ„Ÿï¼‰
- âœ… Lovasz-Hinge Lossï¼ˆIoUç›´æ¥ä¼˜åŒ–ï¼‰
- âœ… ç»„åˆæŸå¤±ï¼ˆL = 0.4Â·Dice + 0.3Â·Focal + 0.2Â·BCE + 0.1Â·Boundaryï¼‰

### ğŸš€ è®­ç»ƒä¼˜åŒ–æŠ€æœ¯
- âœ… æ··åˆç²¾åº¦è®­ç»ƒï¼ˆAMPï¼‰
- âœ… æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼ˆEMA, decay=0.9995ï¼‰
- âœ… éšæœºæƒé‡å¹³å‡ï¼ˆSWA, æœ€å10% epochï¼‰
- âœ… æ¢¯åº¦ç´¯ç§¯ä¸è£å‰ª
- âœ… OneCycle / Cosine å­¦ä¹ ç‡è°ƒåº¦
- âœ… æ—©åœæœºåˆ¶ï¼ˆpatience=20ï¼‰

### ğŸ¨ é«˜æ€§èƒ½æ¨ç†
- âœ… æ»‘çª—æ¨ç†ï¼ˆé«˜åˆ†è¾¨ç‡å›¾åƒï¼ŒGaussianèåˆï¼‰
- âœ… æµ‹è¯•æ—¶å¢å¼ºï¼ˆTTAï¼‰ï¼šå¤šå°ºåº¦ + ç¿»è½¬
- âœ… æ¸©åº¦æ ‡å®šï¼ˆTemperature Scalingï¼‰
- âœ… ONNXå¯¼å‡ºï¼ˆopset 17+ï¼‰
- âœ… æ¨¡å‹é‡åŒ–ï¼ˆINT8ï¼‰
- âœ… TensorRTä¼˜åŒ–ï¼ˆFP16/INT8ï¼‰

## ğŸ“ é¡¹ç›®ç»“æ„

```
python-inference/
â”œâ”€â”€ dataset/
â”‚   â””â”€â”€ data_loader.py          # æ•°æ®åŠ è½½å™¨ã€æ ¼å¼è½¬æ¢ã€è´¨é‡æ§åˆ¶
â”œâ”€â”€ models/
â”‚   â””â”€â”€ convnext_upernet.py     # ConvNeXt + UPerNetæ¨¡å‹
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ losses.py               # æŸå¤±å‡½æ•°é›†åˆ
â”‚   â””â”€â”€ trainer.py              # è®­ç»ƒå™¨ï¼ˆEMAã€SWAã€AMPï¼‰
â”œâ”€â”€ inference/
â”‚   â””â”€â”€ sliding_window.py       # æ»‘çª—æ¨ç†ã€TTA
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ train_config.yaml       # è®­ç»ƒé…ç½®
â”œâ”€â”€ train.py                    # è®­ç»ƒå…¥å£
â”œâ”€â”€ export_onnx.py              # ONNXå¯¼å‡ºå·¥å…·
â”œâ”€â”€ requirements.txt            # ä¾èµ–åŒ…
â””â”€â”€ README.md                   # æœ¬æ–‡æ¡£
```

## ğŸ› ï¸ å®‰è£…

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
conda create -n crack-detection python=3.10
conda activate crack-detection

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# å®‰è£…PyTorchï¼ˆæ ¹æ®CUDAç‰ˆæœ¬é€‰æ‹©ï¼‰
# CUDA 11.8
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118

# CUDA 12.1
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
```

## ğŸ“š æ•°æ®å‡†å¤‡

### 1. æ•°æ®é›†ç›®å½•ç»“æ„

```
data/processed/
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ img001.jpg
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ val/
â”‚   â””â”€â”€ test/
â”œâ”€â”€ masks/
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ img001.png  # äºŒå€¼æ©ç ï¼ˆ0/255ï¼‰
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ val/
â”‚   â””â”€â”€ test/
â”œâ”€â”€ train.txt               # è®­ç»ƒé›†æ ·æœ¬åˆ—è¡¨
â”œâ”€â”€ val.txt                 # éªŒè¯é›†æ ·æœ¬åˆ—è¡¨
â””â”€â”€ test.txt                # æµ‹è¯•é›†æ ·æœ¬åˆ—è¡¨
```

### 2. æ ·æœ¬åˆ—è¡¨æ ¼å¼

```txt
# train.txt
img001
img002
img003
...
```

## ğŸš€ è®­ç»ƒ

### å¿«é€Ÿå¼€å§‹

```bash
# ä½¿ç”¨é»˜è®¤é…ç½®è®­ç»ƒ
python train.py --config configs/train_config.yaml

# ä»æ£€æŸ¥ç‚¹æ¢å¤
python train.py --config configs/train_config.yaml --resume outputs/last.pth

# æŒ‡å®šGPU
python train.py --config configs/train_config.yaml --device cuda:0
```

### é…ç½®è¯´æ˜

ç¼–è¾‘ `configs/train_config.yaml`:

```yaml
# æ¨¡å‹é…ç½®
model:
  backbone: "convnext_tiny"      # convnext_tiny, small, base
  decoder_channels: 256
  deep_supervision: true
  edge_branch: true

# è®­ç»ƒé…ç½®
training:
  epochs: 200
  batch_size: 16
  use_amp: true                  # æ··åˆç²¾åº¦
  use_ema: true                  # EMA
  use_swa: true                  # SWA
  swa_start_epoch: 180
```

### è®­ç»ƒæŠ€å·§

1. **å¤šå°ºåº¦è®­ç»ƒ**ï¼š`train_scales: [256, 384, 512]`
2. **åŠ¨æ€å¢å¼ºå¼ºåº¦**ï¼šå‰60% epochå¼ºå¢å¼ºï¼Œå40%å¼±å¢å¼º
3. **éš¾ä¾‹æŒ–æ˜**ï¼šè‡ªåŠ¨æ ¹æ®æŸå¤±å€¼è°ƒæ•´æ ·æœ¬æƒé‡
4. **æ¢¯åº¦ç´¯ç§¯**ï¼šå°æ˜¾å­˜æ—¶å¢å¤§ `gradient_accumulation_steps`

## ğŸ“Š æ¨¡å‹è¯„ä¼°

```python
from training.trainer import Trainer
from models.convnext_upernet import create_model
from training.losses import create_loss

# åŠ è½½æ¨¡å‹
model = create_model(config['model'])
trainer = Trainer(model, optimizer, loss_fn)
trainer.load_checkpoint('outputs/best.pth')

# éªŒè¯
val_metrics = trainer.validate(val_loader)
print(f"Val IoU: {val_metrics['iou']:.4f}")
```

## ğŸ¯ æ¨¡å‹å¯¼å‡º

### å¯¼å‡ºONNX

```bash
# åŸºç¡€å¯¼å‡º
python export_onnx.py \
  --checkpoint outputs/best.pth \
  --output model.onnx \
  --input-shape 1 3 512 512

# å¯¼å‡º + éªŒè¯ + ä¼˜åŒ– + é‡åŒ– + åŸºå‡†æµ‹è¯•
python export_onnx.py \
  --checkpoint outputs/best.pth \
  --output model.onnx \
  --verify \
  --optimize \
  --quantize \
  --benchmark
```

### ONNXæ¨ç†

```python
import onnxruntime as ort
import numpy as np

# åˆ›å»ºä¼šè¯
session = ort.InferenceSession('model.onnx')

# æ¨ç†
input_data = np.random.randn(1, 3, 512, 512).astype(np.float32)
output = session.run(None, {'input': input_data})[0]
```

## ğŸ”¬ é«˜çº§æ¨ç†

### æ»‘çª—æ¨ç†ï¼ˆé«˜åˆ†è¾¨ç‡ï¼‰

```python
from inference.sliding_window import SlidingWindowInference

sliding_window = SlidingWindowInference(
    window_size=(1024, 1024),
    overlap=0.25,
    batch_size=4,
    blend_mode='gaussian'
)

pred = sliding_window(model, high_res_image, device='cuda')
```

### æµ‹è¯•æ—¶å¢å¼ºï¼ˆTTAï¼‰

```python
from inference.sliding_window import TTAInference

tta = TTAInference(
    scales=[0.75, 1.0, 1.25],
    flip_h=True,
    flip_v=True
)

pred = tta(model, image, device='cuda')
```

## ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡

### ç²¾åº¦æŒ‡æ ‡
- **mIoU**: â‰¥ 85%
- **Boundary F1**: è¾¹ç•Œç²¾åº¦è¯„ä¼°
- **Thin-Region IoU**: ç»†è£‚çº¹ï¼ˆå®½åº¦<3pxï¼‰è¯„ä¼°

### é€Ÿåº¦æŒ‡æ ‡
- **è®­ç»ƒé€Ÿåº¦**: ~200 images/s (V100, batch=16, AMP)
- **æ¨ç†é€Ÿåº¦**: 
  - FP32: ~50 ms/image (512Ã—512)
  - FP16: ~30 ms/image
  - INT8: ~20 ms/image

## ğŸ“ å‚è€ƒè®ºæ–‡

1. **ConvNeXt**: A ConvNet for the 2020s ([arXiv](https://arxiv.org/abs/2201.03545))
2. **UPerNet**: Unified Perceptual Parsing ([arXiv](https://arxiv.org/abs/1807.10221))
3. **CBAM**: Convolutional Block Attention Module ([arXiv](https://arxiv.org/abs/1807.06521))
4. **Focal Loss**: Dense Object Detection ([arXiv](https://arxiv.org/abs/1708.02002))
5. **Lovasz Loss**: A tractable surrogate for IoU optimization ([arXiv](https://arxiv.org/abs/1705.08790))

## ğŸ“š è¯¦ç»†æ–‡æ¡£

- [å®ç°æ€»ç»“](docs/å®ç°æ€»ç»“.md) - æŠ€æœ¯å®ç°ç»†èŠ‚å’Œç®—æ³•åŸç†
- [æ•°æ®å¤„ç†æŒ‡å—](docs/æ•°æ®å¤„ç†æŒ‡å—.md) - æ•°æ®é›†è§„èŒƒåŒ–å’Œå¤„ç†æµç¨‹
- [é¡¹ç›®ä¸»æ–‡æ¡£](../README.md) - é¡¹ç›®æ¦‚è¿°å’Œå¿«é€Ÿå¼€å§‹
- [ç³»ç»Ÿè®¾è®¡æ–¹æ¡ˆ](../docs/ç³»ç»Ÿè®¾è®¡æ–¹æ¡ˆ.md) - å®Œæ•´ç³»ç»Ÿæ¶æ„
- [ä½¿ç”¨æŒ‡å—](../docs/ä½¿ç”¨æŒ‡å—.md) - è¯¦ç»†ä½¿ç”¨æ•™ç¨‹

## ğŸ“ è®¸å¯è¯

MIT License

## ğŸ‘¥ è”ç³»æ–¹å¼

- ä½œè€…ï¼šé«˜ç»…è¯­
- æŒ‡å¯¼æ•™å¸ˆï¼šæ¨é£
- å­¦æ ¡ï¼šå±±ä¸œå¤§å­¦ ä½ç©ºç§‘å­¦ä¸å·¥ç¨‹å­¦é™¢
- ä¸“ä¸šï¼šè®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯ï¼ˆæœªæ¥ç½‘ç»œï¼‰

---

**æ³¨æ„**ï¼šæœ¬é¡¹ç›®ä¸ºæ¯•ä¸šè®¾è®¡é¡¹ç›®ï¼Œä»£ç ä»…ä¾›å­¦ä¹ å‚è€ƒã€‚

